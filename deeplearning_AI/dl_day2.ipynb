{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1026)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('f:/data/wine.csv', header=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset[:,0:12]\n",
    "y=dataset[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(24, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장(검증용 데이터 정확도가 가장 우수한)\n",
    "mymodelpath='./mymodel/'\n",
    "if not os.path.exists(mymodelpath):\n",
    "    os.mkdir(mymodelpath) # 가장 우수한 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.9101\n",
      "Epoch 2/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.9235\n",
      "Epoch 3/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9235\n",
      "Epoch 4/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9332\n",
      "Epoch 5/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9340\n",
      "Epoch 6/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9374\n",
      "Epoch 7/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1726 - accuracy: 0.9395\n",
      "Epoch 8/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9429\n",
      "Epoch 9/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9441\n",
      "Epoch 10/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9451\n",
      "Epoch 11/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9484\n",
      "Epoch 12/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9500\n",
      "Epoch 13/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9491\n",
      "Epoch 14/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9517\n",
      "Epoch 15/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9538\n",
      "Epoch 16/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9543\n",
      "Epoch 17/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9564\n",
      "Epoch 18/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9567\n",
      "Epoch 19/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.1125 - accuracy: 0.9608\n",
      "Epoch 20/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.1102 - accuracy: 0.9604\n",
      "Epoch 21/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.9637\n",
      "Epoch 22/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9621\n",
      "Epoch 23/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9672\n",
      "Epoch 24/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9677\n",
      "Epoch 25/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9681\n",
      "Epoch 26/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9694\n",
      "Epoch 27/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0899 - accuracy: 0.9703\n",
      "Epoch 28/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9689\n",
      "Epoch 29/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9723\n",
      "Epoch 30/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9724\n",
      "Epoch 31/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0821 - accuracy: 0.9749\n",
      "Epoch 32/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9738\n",
      "Epoch 33/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9751\n",
      "Epoch 34/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9746\n",
      "Epoch 35/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0832 - accuracy: 0.9734\n",
      "Epoch 36/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9765\n",
      "Epoch 37/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9755\n",
      "Epoch 38/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9769\n",
      "Epoch 39/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9751\n",
      "Epoch 40/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.9781\n",
      "Epoch 41/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9757\n",
      "Epoch 42/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9777\n",
      "Epoch 43/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9778\n",
      "Epoch 44/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0692 - accuracy: 0.9789\n",
      "Epoch 45/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9772\n",
      "Epoch 46/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9777\n",
      "Epoch 47/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9786\n",
      "Epoch 48/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9795\n",
      "Epoch 49/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9800\n",
      "Epoch 50/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9766\n",
      "Epoch 51/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9786\n",
      "Epoch 52/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9774\n",
      "Epoch 53/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9786\n",
      "Epoch 54/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9785\n",
      "Epoch 55/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9791\n",
      "Epoch 56/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9808\n",
      "Epoch 57/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9811\n",
      "Epoch 58/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9798\n",
      "Epoch 59/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.9792\n",
      "Epoch 60/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9806\n",
      "Epoch 61/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9795\n",
      "Epoch 62/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9809\n",
      "Epoch 63/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9801\n",
      "Epoch 64/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.9794\n",
      "Epoch 65/200\n",
      "65/65 [==============================] - 0s 921us/step - loss: 0.0633 - accuracy: 0.9803\n",
      "Epoch 66/200\n",
      "65/65 [==============================] - 0s 967us/step - loss: 0.0649 - accuracy: 0.9808\n",
      "Epoch 67/200\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0608 - accuracy: 0.9828\n",
      "Epoch 68/200\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0600 - accuracy: 0.9825\n",
      "Epoch 69/200\n",
      "65/65 [==============================] - 0s 967us/step - loss: 0.0664 - accuracy: 0.9803\n",
      "Epoch 70/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9789\n",
      "Epoch 71/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9766\n",
      "Epoch 72/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9808\n",
      "Epoch 73/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.9811\n",
      "Epoch 74/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9823\n",
      "Epoch 75/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.9818\n",
      "Epoch 76/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9829\n",
      "Epoch 77/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.9825\n",
      "Epoch 78/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9809\n",
      "Epoch 79/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9806\n",
      "Epoch 80/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9811\n",
      "Epoch 81/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9828\n",
      "Epoch 82/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9838\n",
      "Epoch 83/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0586 - accuracy: 0.9828\n",
      "Epoch 84/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9818\n",
      "Epoch 85/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9829\n",
      "Epoch 86/200\n",
      "65/65 [==============================] - 0s 951us/step - loss: 0.0559 - accuracy: 0.9840\n",
      "Epoch 87/200\n",
      "65/65 [==============================] - 0s 921us/step - loss: 0.0595 - accuracy: 0.9811\n",
      "Epoch 88/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9815\n",
      "Epoch 89/200\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0591 - accuracy: 0.9823\n",
      "Epoch 90/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0595 - accuracy: 0.9821\n",
      "Epoch 91/200\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0570 - accuracy: 0.9831\n",
      "Epoch 92/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9823\n",
      "Epoch 93/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9825\n",
      "Epoch 94/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0563 - accuracy: 0.9835\n",
      "Epoch 95/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9828\n",
      "Epoch 96/200\n",
      "65/65 [==============================] - 0s 951us/step - loss: 0.0561 - accuracy: 0.9834\n",
      "Epoch 97/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9840\n",
      "Epoch 98/200\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0621 - accuracy: 0.9817\n",
      "Epoch 99/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0568 - accuracy: 0.9829\n",
      "Epoch 100/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9845\n",
      "Epoch 101/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9831\n",
      "Epoch 102/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9826\n",
      "Epoch 103/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9821\n",
      "Epoch 104/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9841\n",
      "Epoch 105/200\n",
      "65/65 [==============================] - 0s 981us/step - loss: 0.0572 - accuracy: 0.9806\n",
      "Epoch 106/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9857\n",
      "Epoch 107/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0548 - accuracy: 0.9837\n",
      "Epoch 108/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9834\n",
      "Epoch 109/200\n",
      "65/65 [==============================] - 0s 989us/step - loss: 0.0613 - accuracy: 0.9808\n",
      "Epoch 110/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9829\n",
      "Epoch 111/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9831\n",
      "Epoch 112/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9837\n",
      "Epoch 113/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9837\n",
      "Epoch 114/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9818\n",
      "Epoch 115/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9832\n",
      "Epoch 116/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9798\n",
      "Epoch 117/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9818\n",
      "Epoch 118/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0536 - accuracy: 0.9843\n",
      "Epoch 119/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9829\n",
      "Epoch 120/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9841\n",
      "Epoch 121/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.9815\n",
      "Epoch 122/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9838\n",
      "Epoch 123/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9817\n",
      "Epoch 124/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9840\n",
      "Epoch 125/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9828\n",
      "Epoch 126/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9834\n",
      "Epoch 127/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9848\n",
      "Epoch 128/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9841\n",
      "Epoch 129/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9809\n",
      "Epoch 130/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.9815\n",
      "Epoch 131/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0525 - accuracy: 0.9826\n",
      "Epoch 132/200\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0527 - accuracy: 0.9823\n",
      "Epoch 133/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9843\n",
      "Epoch 134/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9851\n",
      "Epoch 135/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0550 - accuracy: 0.9848\n",
      "Epoch 136/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9808\n",
      "Epoch 137/200\n",
      "65/65 [==============================] - 0s 977us/step - loss: 0.0521 - accuracy: 0.9855\n",
      "Epoch 138/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9840\n",
      "Epoch 139/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9855\n",
      "Epoch 140/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9823\n",
      "Epoch 141/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9825\n",
      "Epoch 142/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0559 - accuracy: 0.9815\n",
      "Epoch 143/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9841\n",
      "Epoch 144/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9863\n",
      "Epoch 145/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0520 - accuracy: 0.9851\n",
      "Epoch 146/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9840\n",
      "Epoch 147/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9814\n",
      "Epoch 148/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9854\n",
      "Epoch 149/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9829\n",
      "Epoch 150/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9852\n",
      "Epoch 151/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0507 - accuracy: 0.9860\n",
      "Epoch 152/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 0.9848\n",
      "Epoch 153/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0507 - accuracy: 0.9851\n",
      "Epoch 154/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9858\n",
      "Epoch 155/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9835\n",
      "Epoch 156/200\n",
      "65/65 [==============================] - 0s 998us/step - loss: 0.0529 - accuracy: 0.9834\n",
      "Epoch 157/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9820\n",
      "Epoch 158/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0536 - accuracy: 0.9838\n",
      "Epoch 159/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0523 - accuracy: 0.9854\n",
      "Epoch 160/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9825\n",
      "Epoch 161/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9855\n",
      "Epoch 162/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9855\n",
      "Epoch 163/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9843\n",
      "Epoch 164/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9846\n",
      "Epoch 165/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9825\n",
      "Epoch 166/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9821\n",
      "Epoch 167/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0540 - accuracy: 0.9837\n",
      "Epoch 168/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9848\n",
      "Epoch 169/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9846\n",
      "Epoch 170/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9843\n",
      "Epoch 171/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9843\n",
      "Epoch 172/200\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0515 - accuracy: 0.9843\n",
      "Epoch 173/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9845\n",
      "Epoch 174/200\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0691 - accuracy: 0.9771\n",
      "Epoch 175/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9866\n",
      "Epoch 176/200\n",
      "65/65 [==============================] - 0s 966us/step - loss: 0.0606 - accuracy: 0.9821\n",
      "Epoch 177/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.9834\n",
      "Epoch 178/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9852\n",
      "Epoch 179/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 0.9846\n",
      "Epoch 180/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9851\n",
      "Epoch 181/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9837\n",
      "Epoch 182/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0520 - accuracy: 0.9855\n",
      "Epoch 183/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9820\n",
      "Epoch 184/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9843\n",
      "Epoch 185/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9843\n",
      "Epoch 186/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.9857\n",
      "Epoch 187/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9851\n",
      "Epoch 188/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9848\n",
      "Epoch 189/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9846\n",
      "Epoch 190/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9837\n",
      "Epoch 191/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0513 - accuracy: 0.9849\n",
      "Epoch 192/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0513 - accuracy: 0.9845\n",
      "Epoch 193/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 0.9861\n",
      "Epoch 194/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9855\n",
      "Epoch 195/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0505 - accuracy: 0.9854\n",
      "Epoch 196/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0505 - accuracy: 0.9865\n",
      "Epoch 197/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9854\n",
      "Epoch 198/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0540 - accuracy: 0.9840\n",
      "Epoch 199/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9857\n",
      "Epoch 200/200\n",
      "65/65 [==============================] - 0s 980us/step - loss: 0.0491 - accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2711b1c0f48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y, epochs=200, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 : save(), 불러오기 : load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1026)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2   3      4      5     6       7   8      9     10  \\\n",
       "0    0.00632  18.0   2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1    0.02731   0.0   7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2    0.02729   0.0   7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3    0.03237   0.0   2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4    0.06905   0.0   2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "..       ...   ...    ...  ..    ...    ...   ...     ...  ..    ...   ...   \n",
       "501  0.06263   0.0  11.93   0  0.573  6.593  69.1  2.4786   1  273.0  21.0   \n",
       "502  0.04527   0.0  11.93   0  0.573  6.120  76.7  2.2875   1  273.0  21.0   \n",
       "503  0.06076   0.0  11.93   0  0.573  6.976  91.0  2.1675   1  273.0  21.0   \n",
       "504  0.10959   0.0  11.93   0  0.573  6.794  89.3  2.3889   1  273.0  21.0   \n",
       "505  0.04741   0.0  11.93   0  0.573  6.030  80.8  2.5050   1  273.0  21.0   \n",
       "\n",
       "         11    12    13  \n",
       "0    396.90  4.98  24.0  \n",
       "1    396.90  9.14  21.6  \n",
       "2    392.83  4.03  34.7  \n",
       "3    394.63  2.94  33.4  \n",
       "4    396.90  5.33  36.2  \n",
       "..      ...   ...   ...  \n",
       "501  391.99  9.67  22.4  \n",
       "502  396.90  9.08  20.6  \n",
       "503  396.90  5.64  23.9  \n",
       "504  393.45  6.48  22.0  \n",
       "505  396.90  7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('f:/data/housing.csv', header=None, delim_whitespace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.values[:,0:13]\n",
    "y=df.values[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=13, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1)) # 연속형 값으로 예측되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 376.4956\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 128.8094\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 103.5719\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 94.2299\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 942us/step - loss: 87.1038\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 79.3588\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 72.4685\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 72.7765\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 65.1325\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 61.4827\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 58.0132\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 56.1329\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 55.3919\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.9090\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.8499\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 970us/step - loss: 50.5668\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.7858\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 49.1475\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 942us/step - loss: 46.3729\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.9197\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 915us/step - loss: 47.0313\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 776us/step - loss: 43.0384\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 720us/step - loss: 43.3082\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 836us/step - loss: 41.4763\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 844us/step - loss: 40.3871\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 789us/step - loss: 41.0593\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 830us/step - loss: 41.7290\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 898us/step - loss: 42.1090\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 878us/step - loss: 41.3240\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 852us/step - loss: 43.7561\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 807us/step - loss: 41.1385\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 997us/step - loss: 39.9340\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.1856\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 885us/step - loss: 38.3556\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 877us/step - loss: 37.1073\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 859us/step - loss: 37.2760\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 836us/step - loss: 39.9679\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 820us/step - loss: 37.7225\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 859us/step - loss: 37.0305\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 875us/step - loss: 37.6558\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 848us/step - loss: 35.1507\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 820us/step - loss: 36.6024\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 871us/step - loss: 36.4395\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 865us/step - loss: 34.5586\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 882us/step - loss: 38.7292\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 852us/step - loss: 42.4488\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 850us/step - loss: 34.5159\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 835us/step - loss: 39.2887\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 42.6814\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 826us/step - loss: 36.3989\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 851us/step - loss: 37.8464\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 884us/step - loss: 34.1971\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 840us/step - loss: 34.0037\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 803us/step - loss: 37.2433\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 824us/step - loss: 34.2675\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 33.4747\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 970us/step - loss: 33.1276\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.6948\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 952us/step - loss: 35.4956\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 887us/step - loss: 33.6938\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 875us/step - loss: 37.8401\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 834us/step - loss: 33.1108\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 830us/step - loss: 34.7793\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 853us/step - loss: 34.1807\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 823us/step - loss: 36.2328\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 807us/step - loss: 33.9597\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 869us/step - loss: 33.4400\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 875us/step - loss: 33.3468\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 943us/step - loss: 35.0432\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.6707\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.1497\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 855us/step - loss: 33.5450\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 926us/step - loss: 33.0850\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 942us/step - loss: 38.1711\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 885us/step - loss: 34.8589\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 858us/step - loss: 31.9239\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 809us/step - loss: 33.0823\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 840us/step - loss: 34.2848\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 853us/step - loss: 34.0764\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 943us/step - loss: 32.9833\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 860us/step - loss: 31.6937\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 859us/step - loss: 33.9250\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 832us/step - loss: 32.3596\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 826us/step - loss: 38.4592\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 858us/step - loss: 33.3769\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 856us/step - loss: 35.4407\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 847us/step - loss: 36.5847\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.9185\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 997us/step - loss: 31.8641\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 870us/step - loss: 32.2065\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 871us/step - loss: 34.1984\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 827us/step - loss: 31.7384\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 33.2076\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 832us/step - loss: 37.7938\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 870us/step - loss: 32.0959\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 849us/step - loss: 33.4385\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 846us/step - loss: 35.5254\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 0s 870us/step - loss: 34.4294\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.5283\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.5507\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 922us/step - loss: 34.8273\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 834us/step - loss: 32.0360\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 832us/step - loss: 34.0429\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 962us/step - loss: 31.0838\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.9755\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.6649\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 891us/step - loss: 30.5877\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 870us/step - loss: 31.3913\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 843us/step - loss: 34.5105\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 970us/step - loss: 34.7577\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.7580\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 962us/step - loss: 33.5118\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 847us/step - loss: 30.7900\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 869us/step - loss: 29.7450\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 919us/step - loss: 30.9097\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 840us/step - loss: 32.2132\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 897us/step - loss: 29.4161\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 921us/step - loss: 31.1412\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 885us/step - loss: 30.0075\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 845us/step - loss: 34.1235\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 888us/step - loss: 32.8107\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 954us/step - loss: 29.4206\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 905us/step - loss: 36.8174\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 898us/step - loss: 35.8489\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.1860\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 1000us/step - loss: 31.1713\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 946us/step - loss: 31.6333\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 939us/step - loss: 28.9675\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 907us/step - loss: 31.3981\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 919us/step - loss: 32.6840\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 953us/step - loss: 29.6809\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.8193\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 924us/step - loss: 31.3800\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 923us/step - loss: 30.1251\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 918us/step - loss: 28.8359\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 913us/step - loss: 32.8908\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 925us/step - loss: 29.5259\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 29.6285\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 920us/step - loss: 29.0544\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 942us/step - loss: 30.7714\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 926us/step - loss: 28.9470\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 928us/step - loss: 31.5970\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 912us/step - loss: 30.0209\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 895us/step - loss: 28.8938\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 873us/step - loss: 28.8879\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 886us/step - loss: 28.0783\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 952us/step - loss: 28.8919\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 905us/step - loss: 32.0009\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 934us/step - loss: 30.6394\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 924us/step - loss: 28.3211\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 913us/step - loss: 29.3681\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 887us/step - loss: 28.8372\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 883us/step - loss: 28.7952\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 921us/step - loss: 29.6251\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 28.1977\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 926us/step - loss: 28.0813\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 780us/step - loss: 28.3743\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 776us/step - loss: 29.3610\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 29.9191\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 804us/step - loss: 29.3874\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 833us/step - loss: 29.7395\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 803us/step - loss: 27.8525\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 830us/step - loss: 28.1094\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 803us/step - loss: 32.2648\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 830us/step - loss: 33.2857\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 828us/step - loss: 28.3808\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 796us/step - loss: 29.0644\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 803us/step - loss: 36.4291\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 848us/step - loss: 31.6707\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 832us/step - loss: 28.9537\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 33.9152\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 29.0704\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 31.6915\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 811us/step - loss: 31.2869\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 807us/step - loss: 32.1474\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 798us/step - loss: 27.9676\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 804us/step - loss: 32.3590\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 915us/step - loss: 28.7118\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 886us/step - loss: 28.0454\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.1887\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.4515\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 959us/step - loss: 27.1248\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.1219\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 865us/step - loss: 28.7858\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 804us/step - loss: 27.3739\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 830us/step - loss: 28.2405\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 847us/step - loss: 30.6613\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 798us/step - loss: 30.6088\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 804us/step - loss: 26.8122\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 942us/step - loss: 28.0013\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.4119\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 914us/step - loss: 27.5880\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 848us/step - loss: 26.9959\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 0s 803us/step - loss: 28.5534\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 832us/step - loss: 31.9316\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 821us/step - loss: 31.7597\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 830us/step - loss: 31.2990\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 896us/step - loss: 29.8590\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 859us/step - loss: 39.8625\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 831us/step - loss: 34.9593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x271244a1f88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.039536 ],\n",
       "       [17.441004 ],\n",
       "       [18.43188  ],\n",
       "       [22.26613  ],\n",
       "       [20.343018 ],\n",
       "       [33.42148  ],\n",
       "       [35.193115 ],\n",
       "       [12.836279 ],\n",
       "       [27.33987  ],\n",
       "       [30.0772   ],\n",
       "       [23.671642 ],\n",
       "       [34.29325  ],\n",
       "       [18.545687 ],\n",
       "       [ 4.554568 ],\n",
       "       [25.941416 ],\n",
       "       [26.451925 ],\n",
       "       [ 9.8055   ],\n",
       "       [ 6.546353 ],\n",
       "       [29.736052 ],\n",
       "       [13.363943 ],\n",
       "       [21.107368 ],\n",
       "       [32.451614 ],\n",
       "       [20.616936 ],\n",
       "       [24.407974 ],\n",
       "       [10.754831 ],\n",
       "       [16.981676 ],\n",
       "       [17.241518 ],\n",
       "       [17.173365 ],\n",
       "       [25.209171 ],\n",
       "       [15.2593155],\n",
       "       [31.146666 ],\n",
       "       [18.67236  ],\n",
       "       [25.463379 ],\n",
       "       [29.90106  ],\n",
       "       [30.6633   ],\n",
       "       [19.012774 ],\n",
       "       [ 7.454245 ],\n",
       "       [23.447063 ],\n",
       "       [20.946465 ],\n",
       "       [24.814243 ],\n",
       "       [ 1.111153 ],\n",
       "       [19.95374  ],\n",
       "       [28.377497 ],\n",
       "       [24.072529 ],\n",
       "       [37.77258  ],\n",
       "       [26.679775 ],\n",
       "       [26.750916 ],\n",
       "       [35.886707 ],\n",
       "       [26.937777 ],\n",
       "       [21.76337  ],\n",
       "       [25.66669  ],\n",
       "       [22.376617 ],\n",
       "       [20.865887 ],\n",
       "       [24.259382 ],\n",
       "       [24.551178 ],\n",
       "       [29.210964 ],\n",
       "       [16.108894 ],\n",
       "       [24.564632 ],\n",
       "       [23.75262  ],\n",
       "       [16.013529 ],\n",
       "       [12.23273  ],\n",
       "       [33.502323 ],\n",
       "       [26.248299 ],\n",
       "       [20.11686  ],\n",
       "       [16.484545 ],\n",
       "       [17.694729 ],\n",
       "       [21.303867 ],\n",
       "       [26.624441 ],\n",
       "       [10.467731 ],\n",
       "       [20.778616 ],\n",
       "       [25.753448 ],\n",
       "       [33.576897 ],\n",
       "       [30.455015 ],\n",
       "       [29.176647 ],\n",
       "       [ 7.0073113],\n",
       "       [20.676947 ],\n",
       "       [25.921053 ],\n",
       "       [29.986217 ],\n",
       "       [20.056118 ],\n",
       "       [30.511374 ],\n",
       "       [17.162983 ],\n",
       "       [30.797813 ],\n",
       "       [26.356789 ],\n",
       "       [29.594402 ],\n",
       "       [10.798932 ],\n",
       "       [26.894842 ],\n",
       "       [ 2.9200292],\n",
       "       [35.7692   ],\n",
       "       [15.214716 ],\n",
       "       [25.069946 ],\n",
       "       [25.956514 ],\n",
       "       [30.472145 ],\n",
       "       [-2.5613475],\n",
       "       [19.110544 ],\n",
       "       [15.7100525],\n",
       "       [23.605356 ],\n",
       "       [13.917023 ],\n",
       "       [30.584023 ],\n",
       "       [32.275566 ],\n",
       "       [14.03948  ],\n",
       "       [19.983406 ],\n",
       "       [24.496227 ],\n",
       "       [17.743505 ],\n",
       "       [34.767166 ],\n",
       "       [20.580803 ],\n",
       "       [17.823275 ],\n",
       "       [19.799854 ],\n",
       "       [15.4246025],\n",
       "       [18.443323 ],\n",
       "       [15.434597 ],\n",
       "       [28.804108 ],\n",
       "       [24.709435 ],\n",
       "       [26.207163 ],\n",
       "       [26.113016 ],\n",
       "       [28.413542 ],\n",
       "       [23.446314 ],\n",
       "       [14.404261 ],\n",
       "       [22.585232 ],\n",
       "       [10.322659 ],\n",
       "       [19.095428 ],\n",
       "       [ 1.6239704],\n",
       "       [34.809013 ],\n",
       "       [26.37687  ],\n",
       "       [24.231325 ],\n",
       "       [18.71577  ],\n",
       "       [20.06295  ],\n",
       "       [ 8.678022 ],\n",
       "       [22.98251  ],\n",
       "       [ 8.563797 ],\n",
       "       [10.97858  ],\n",
       "       [23.532751 ],\n",
       "       [14.466112 ],\n",
       "       [ 8.920521 ],\n",
       "       [21.527563 ],\n",
       "       [23.225084 ],\n",
       "       [13.763433 ],\n",
       "       [27.043549 ],\n",
       "       [28.786526 ],\n",
       "       [17.678118 ],\n",
       "       [29.404644 ],\n",
       "       [18.576813 ],\n",
       "       [30.067717 ],\n",
       "       [26.413752 ],\n",
       "       [32.922    ],\n",
       "       [39.267612 ],\n",
       "       [26.1993   ],\n",
       "       [17.804209 ],\n",
       "       [10.003038 ],\n",
       "       [17.914967 ],\n",
       "       [20.931396 ],\n",
       "       [30.576843 ],\n",
       "       [ 7.2486653]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(xtest).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제:21.500, 예상:22.040\n",
      "실제:23.200, 예상:17.441\n",
      "실제:16.700, 예상:18.432\n",
      "실제:23.100, 예상:22.266\n",
      "실제:21.800, 예상:20.343\n",
      "실제:30.800, 예상:33.421\n",
      "실제:37.300, 예상:35.193\n",
      "실제:10.900, 예상:12.836\n",
      "실제:22.200, 예상:27.340\n",
      "실제:33.200, 예상:30.077\n",
      "실제:20.600, 예상:23.672\n",
      "실제:35.400, 예상:34.293\n",
      "실제:20.300, 예상:18.546\n",
      "실제:13.500, 예상:4.555\n",
      "실제:20.700, 예상:25.941\n",
      "실제:27.000, 예상:26.452\n",
      "실제:20.200, 예상:9.806\n",
      "실제:10.200, 예상:6.546\n",
      "실제:34.900, 예상:29.736\n",
      "실제:8.500, 예상:13.364\n",
      "실제:27.900, 예상:21.107\n",
      "실제:48.300, 예상:32.452\n",
      "실제:16.500, 예상:20.617\n",
      "실제:25.000, 예상:24.408\n",
      "실제:16.500, 예상:10.755\n",
      "실제:17.800, 예상:16.982\n",
      "실제:14.300, 예상:17.242\n",
      "실제:19.300, 예상:17.173\n",
      "실제:22.000, 예상:25.209\n",
      "실제:12.700, 예상:15.259\n",
      "실제:36.200, 예상:31.147\n",
      "실제:21.900, 예상:18.672\n",
      "실제:23.900, 예상:25.463\n",
      "실제:31.500, 예상:29.901\n",
      "실제:42.300, 예상:30.663\n",
      "실제:15.000, 예상:19.013\n",
      "실제:5.600, 예상:7.454\n",
      "실제:24.400, 예상:23.447\n",
      "실제:18.200, 예상:20.946\n",
      "실제:23.300, 예상:24.814\n",
      "실제:7.400, 예상:1.111\n",
      "실제:18.300, 예상:19.954\n",
      "실제:28.000, 예상:28.377\n",
      "실제:20.100, 예상:24.073\n",
      "실제:43.500, 예상:37.773\n",
      "실제:23.400, 예상:26.680\n",
      "실제:23.900, 예상:26.751\n",
      "실제:34.900, 예상:35.887\n",
      "실제:22.600, 예상:26.938\n",
      "실제:19.600, 예상:21.763\n",
      "실제:18.900, 예상:25.667\n",
      "실제:20.000, 예상:22.377\n",
      "실제:22.200, 예상:20.866\n",
      "실제:22.700, 예상:24.259\n",
      "실제:29.600, 예상:24.551\n",
      "실제:33.100, 예상:29.211\n",
      "실제:14.500, 예상:16.109\n",
      "실제:21.200, 예상:24.565\n",
      "실제:20.900, 예상:23.753\n",
      "실제:19.300, 예상:16.014\n",
      "실제:14.500, 예상:12.233\n",
      "실제:50.000, 예상:33.502\n",
      "실제:21.100, 예상:26.248\n",
      "실제:20.300, 예상:20.117\n",
      "실제:27.500, 예상:16.485\n",
      "실제:19.500, 예상:17.695\n",
      "실제:22.400, 예상:21.304\n",
      "실제:28.600, 예상:26.624\n",
      "실제:17.800, 예상:10.468\n",
      "실제:21.800, 예상:20.779\n",
      "실제:23.000, 예상:25.753\n",
      "실제:50.000, 예상:33.577\n",
      "실제:36.200, 예상:30.455\n",
      "실제:30.700, 예상:29.177\n",
      "실제:7.500, 예상:7.007\n",
      "실제:19.300, 예상:20.677\n",
      "실제:23.800, 예상:25.921\n",
      "실제:29.900, 예상:29.986\n",
      "실제:19.400, 예상:20.056\n",
      "실제:27.500, 예상:30.511\n",
      "실제:16.600, 예상:17.163\n",
      "실제:46.700, 예상:30.798\n",
      "실제:21.600, 예상:26.357\n",
      "실제:24.500, 예상:29.594\n",
      "실제:13.400, 예상:10.799\n",
      "실제:20.600, 예상:26.895\n",
      "실제:17.900, 예상:2.920\n",
      "실제:32.400, 예상:35.769\n",
      "실제:15.000, 예상:15.215\n",
      "실제:22.900, 예상:25.070\n",
      "실제:20.500, 예상:25.957\n",
      "실제:34.700, 예상:30.472\n",
      "실제:13.200, 예상:-2.561\n",
      "실제:29.800, 예상:19.111\n",
      "실제:15.200, 예상:15.710\n",
      "실제:27.500, 예상:23.605\n",
      "실제:16.100, 예상:13.917\n",
      "실제:23.600, 예상:30.584\n",
      "실제:34.900, 예상:32.276\n",
      "실제:23.100, 예상:14.039\n",
      "실제:19.600, 예상:19.983\n",
      "실제:23.900, 예상:24.496\n",
      "실제:21.400, 예상:17.744\n",
      "실제:36.000, 예상:34.767\n",
      "실제:19.100, 예상:20.581\n",
      "실제:17.800, 예상:17.823\n",
      "실제:19.300, 예상:19.800\n",
      "실제:19.100, 예상:15.425\n",
      "실제:17.500, 예상:18.443\n",
      "실제:15.600, 예상:15.435\n",
      "실제:23.600, 예상:28.804\n",
      "실제:20.300, 예상:24.709\n",
      "실제:24.600, 예상:26.207\n",
      "실제:23.900, 예상:26.113\n",
      "실제:25.000, 예상:28.414\n",
      "실제:27.500, 예상:23.446\n",
      "실제:16.100, 예상:14.404\n",
      "실제:20.400, 예상:22.585\n",
      "실제:13.300, 예상:10.323\n",
      "실제:18.900, 예상:19.095\n",
      "실제:6.300, 예상:1.624\n",
      "실제:37.600, 예상:34.809\n",
      "실제:23.700, 예상:26.377\n",
      "실제:11.900, 예상:24.231\n",
      "실제:18.100, 예상:18.716\n",
      "실제:22.300, 예상:20.063\n",
      "실제:11.000, 예상:8.678\n",
      "실제:22.900, 예상:22.983\n",
      "실제:17.200, 예상:8.564\n",
      "실제:14.900, 예상:10.979\n",
      "실제:18.900, 예상:23.533\n",
      "실제:17.400, 예상:14.466\n",
      "실제:10.500, 예상:8.921\n",
      "실제:22.000, 예상:21.528\n",
      "실제:22.600, 예상:23.225\n",
      "실제:14.300, 예상:13.763\n",
      "실제:23.700, 예상:27.044\n",
      "실제:32.000, 예상:28.787\n",
      "실제:23.800, 예상:17.678\n",
      "실제:24.400, 예상:29.405\n",
      "실제:25.000, 예상:18.577\n",
      "실제:29.400, 예상:30.068\n",
      "실제:22.000, 예상:26.414\n",
      "실제:35.200, 예상:32.922\n",
      "실제:50.000, 예상:39.268\n",
      "실제:20.700, 예상:26.199\n",
      "실제:23.100, 예상:17.804\n",
      "실제:19.000, 예상:10.003\n",
      "실제:16.200, 예상:17.915\n",
      "실제:24.400, 예상:20.931\n",
      "실제:29.100, 예상:30.577\n",
      "실제:11.700, 예상:7.249\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    print('실제:{:.3f}, 예상:{:.3f}'.format(ytest[i], pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1026)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702e2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1=pd.read_csv(\"wine.csv\", header=None) #6498\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d277004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c67254",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset[:,0:12]\n",
    "y=dataset[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(32,input_dim=12 , activation='relu' ))\n",
    "model.add(Dense(16, activation='relu' ))\n",
    "model.add(Dense(8, activation='relu' ))\n",
    "model.add(Dense(1, activation='sigmoid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85337713",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodelpath=\"./mymodel/\"\n",
    "if not os.path.exists(mymodelpath):\n",
    "    os.mkdir(mymodelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장(검증용 데이터 정확도가 가장 우수한)\n",
    "modelpath=\"./mymodel/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51638a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "65/65 [==============================] - 1s 1ms/step - loss: 0.4649 - accuracy: 0.8539\n",
      "Epoch 2/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9349\n",
      "Epoch 3/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9386\n",
      "Epoch 4/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9418\n",
      "Epoch 5/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9451\n",
      "Epoch 6/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9475\n",
      "Epoch 7/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.9507\n",
      "Epoch 8/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9532\n",
      "Epoch 9/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9557\n",
      "Epoch 10/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9606\n",
      "Epoch 11/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9646\n",
      "Epoch 12/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9641\n",
      "Epoch 13/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9680\n",
      "Epoch 14/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9712\n",
      "Epoch 15/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9691\n",
      "Epoch 16/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9743\n",
      "Epoch 17/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9755\n",
      "Epoch 18/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9744\n",
      "Epoch 19/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9769\n",
      "Epoch 20/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9681\n",
      "Epoch 21/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9774\n",
      "Epoch 22/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9785\n",
      "Epoch 23/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9754\n",
      "Epoch 24/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9766\n",
      "Epoch 25/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9803\n",
      "Epoch 26/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9792\n",
      "Epoch 27/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9800\n",
      "Epoch 28/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9786\n",
      "Epoch 29/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9792\n",
      "Epoch 30/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9791\n",
      "Epoch 31/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9786\n",
      "Epoch 32/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9808\n",
      "Epoch 33/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.9794\n",
      "Epoch 34/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.9800\n",
      "Epoch 35/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9803\n",
      "Epoch 36/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9805\n",
      "Epoch 37/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9811\n",
      "Epoch 38/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0618 - accuracy: 0.9811\n",
      "Epoch 39/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9818\n",
      "Epoch 40/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9800\n",
      "Epoch 41/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9814\n",
      "Epoch 42/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.9817\n",
      "Epoch 43/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9831\n",
      "Epoch 44/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0582 - accuracy: 0.9820\n",
      "Epoch 45/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9831\n",
      "Epoch 46/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9840\n",
      "Epoch 47/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9791\n",
      "Epoch 48/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9831\n",
      "Epoch 49/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9821\n",
      "Epoch 50/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9808\n",
      "Epoch 51/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9846\n",
      "Epoch 52/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9835\n",
      "Epoch 53/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9848\n",
      "Epoch 54/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9835\n",
      "Epoch 55/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9832\n",
      "Epoch 56/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9821\n",
      "Epoch 57/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9846\n",
      "Epoch 58/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0548 - accuracy: 0.9825\n",
      "Epoch 59/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9821\n",
      "Epoch 60/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9823\n",
      "Epoch 61/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 0.9837\n",
      "Epoch 62/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9852\n",
      "Epoch 63/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0485 - accuracy: 0.9854\n",
      "Epoch 64/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9817\n",
      "Epoch 65/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9837\n",
      "Epoch 66/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9831\n",
      "Epoch 67/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9852\n",
      "Epoch 68/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9828\n",
      "Epoch 69/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9848\n",
      "Epoch 70/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9825\n",
      "Epoch 71/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0487 - accuracy: 0.9849\n",
      "Epoch 72/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9854\n",
      "Epoch 73/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0501 - accuracy: 0.9855\n",
      "Epoch 74/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9846\n",
      "Epoch 75/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9860\n",
      "Epoch 76/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0501 - accuracy: 0.9845\n",
      "Epoch 77/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9846\n",
      "Epoch 78/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9835\n",
      "Epoch 79/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9843\n",
      "Epoch 80/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9857\n",
      "Epoch 81/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0504 - accuracy: 0.9851\n",
      "Epoch 82/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9851\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9798\n",
      "Epoch 84/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9841\n",
      "Epoch 85/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9849\n",
      "Epoch 86/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0463 - accuracy: 0.9858\n",
      "Epoch 87/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.9851\n",
      "Epoch 88/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0513 - accuracy: 0.9845\n",
      "Epoch 89/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9851\n",
      "Epoch 90/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0450 - accuracy: 0.9871\n",
      "Epoch 91/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0464 - accuracy: 0.9857\n",
      "Epoch 92/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9872\n",
      "Epoch 93/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9868\n",
      "Epoch 94/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9874\n",
      "Epoch 95/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9872\n",
      "Epoch 96/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9881\n",
      "Epoch 97/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0477 - accuracy: 0.9865\n",
      "Epoch 98/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0452 - accuracy: 0.9872\n",
      "Epoch 99/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9861\n",
      "Epoch 100/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9880\n",
      "Epoch 101/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9874\n",
      "Epoch 102/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9885\n",
      "Epoch 103/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9845\n",
      "Epoch 104/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9883\n",
      "Epoch 105/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9866\n",
      "Epoch 106/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 0.9880\n",
      "Epoch 107/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9894\n",
      "Epoch 108/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9880\n",
      "Epoch 109/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9871\n",
      "Epoch 110/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9874\n",
      "Epoch 111/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0454 - accuracy: 0.9869\n",
      "Epoch 112/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9886\n",
      "Epoch 113/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9881\n",
      "Epoch 114/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9871\n",
      "Epoch 115/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.9861\n",
      "Epoch 116/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 0.9892\n",
      "Epoch 117/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9880\n",
      "Epoch 118/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0410 - accuracy: 0.9883\n",
      "Epoch 119/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9880\n",
      "Epoch 120/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9883\n",
      "Epoch 121/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0405 - accuracy: 0.9889\n",
      "Epoch 122/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9880\n",
      "Epoch 123/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0425 - accuracy: 0.9888\n",
      "Epoch 124/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9895\n",
      "Epoch 125/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9881\n",
      "Epoch 126/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0425 - accuracy: 0.9878\n",
      "Epoch 127/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9880\n",
      "Epoch 128/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9900\n",
      "Epoch 129/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9894\n",
      "Epoch 130/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9895\n",
      "Epoch 131/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0475 - accuracy: 0.9865\n",
      "Epoch 132/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0448 - accuracy: 0.9874\n",
      "Epoch 133/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9868\n",
      "Epoch 134/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0395 - accuracy: 0.9889\n",
      "Epoch 135/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9898\n",
      "Epoch 136/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0449 - accuracy: 0.9875\n",
      "Epoch 137/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9886\n",
      "Epoch 138/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0400 - accuracy: 0.9898\n",
      "Epoch 139/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0392 - accuracy: 0.9898\n",
      "Epoch 140/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9898\n",
      "Epoch 141/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9881\n",
      "Epoch 142/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9886\n",
      "Epoch 143/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9878\n",
      "Epoch 144/200\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9900\n",
      "Epoch 145/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0395 - accuracy: 0.9889\n",
      "Epoch 146/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9886\n",
      "Epoch 147/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0395 - accuracy: 0.9888\n",
      "Epoch 148/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9878\n",
      "Epoch 149/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0393 - accuracy: 0.9891\n",
      "Epoch 150/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0388 - accuracy: 0.9898\n",
      "Epoch 151/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0426 - accuracy: 0.9883\n",
      "Epoch 152/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0369 - accuracy: 0.9900\n",
      "Epoch 153/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0377 - accuracy: 0.9900\n",
      "Epoch 154/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0411 - accuracy: 0.9885\n",
      "Epoch 155/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9878\n",
      "Epoch 156/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0380 - accuracy: 0.9892\n",
      "Epoch 157/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9901\n",
      "Epoch 158/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9888\n",
      "Epoch 159/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9878\n",
      "Epoch 160/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9891\n",
      "Epoch 161/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9877\n",
      "Epoch 162/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0416 - accuracy: 0.9888\n",
      "Epoch 163/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 0.9888\n",
      "Epoch 164/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0360 - accuracy: 0.9905\n",
      "Epoch 165/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0405 - accuracy: 0.9883\n",
      "Epoch 166/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.9865\n",
      "Epoch 167/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9906\n",
      "Epoch 168/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0372 - accuracy: 0.9895\n",
      "Epoch 169/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0372 - accuracy: 0.9892\n",
      "Epoch 170/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0400 - accuracy: 0.9877\n",
      "Epoch 171/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0359 - accuracy: 0.9900\n",
      "Epoch 172/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0385 - accuracy: 0.9883\n",
      "Epoch 173/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9897\n",
      "Epoch 174/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0356 - accuracy: 0.9903\n",
      "Epoch 175/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0363 - accuracy: 0.9901\n",
      "Epoch 176/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0418 - accuracy: 0.9878\n",
      "Epoch 177/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9889\n",
      "Epoch 178/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0375 - accuracy: 0.9903\n",
      "Epoch 179/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9883\n",
      "Epoch 180/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0359 - accuracy: 0.9901\n",
      "Epoch 181/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9897\n",
      "Epoch 182/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9894\n",
      "Epoch 183/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9900\n",
      "Epoch 184/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.9900\n",
      "Epoch 185/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9891\n",
      "Epoch 186/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9894\n",
      "Epoch 187/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.9906\n",
      "Epoch 188/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9903\n",
      "Epoch 189/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9905\n",
      "Epoch 190/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9908\n",
      "Epoch 191/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0336 - accuracy: 0.9915\n",
      "Epoch 192/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9903\n",
      "Epoch 193/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9903\n",
      "Epoch 194/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0362 - accuracy: 0.9898\n",
      "Epoch 195/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.9915\n",
      "Epoch 196/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0365 - accuracy: 0.9909\n",
      "Epoch 197/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9898\n",
      "Epoch 198/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 0.9923\n",
      "Epoch 199/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9917\n",
      "Epoch 200/200\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296ef18e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(x,y,epochs=200, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa58c0ed82c692c0b88754a3c2a9e69ead6d1ad4625cc9925c53cda4c083e0fe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf2.2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
